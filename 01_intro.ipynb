{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7f40cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the library\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2a1ed79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a blank model to include processing pipeline and language specific \"en\" tokenization rules, exceptions etc.\n",
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17f5597e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Doc object, nlp pass the text to tokenizer and no information is removed from the text\n",
    "doc = nlp(\"this is a test sentences!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f411654c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this\n",
      "is\n",
      "a\n",
      "test\n",
      "sentences\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d49a36d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences\n"
     ]
    }
   ],
   "source": [
    "# access specific token by its index\n",
    "print(doc[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8400ec4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is a test\n"
     ]
    }
   ],
   "source": [
    "# span is contains 1 or more tokens\n",
    "span = doc[1:4]\n",
    "print(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ea986c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token index is 0 \t this           alpha: True \t  punct: False \t     number: False\n",
      "Token index is 1 \t is             alpha: True \t  punct: False \t     number: False\n",
      "Token index is 2 \t a              alpha: True \t  punct: False \t     number: False\n",
      "Token index is 3 \t test           alpha: True \t  punct: False \t     number: False\n",
      "Token index is 4 \t sentences      alpha: True \t  punct: False \t     number: False\n",
      "Token index is 5 \t !              alpha: False \t  punct: True \t     number: False\n"
     ]
    }
   ],
   "source": [
    "# more attributes of a token\n",
    "\n",
    "for token in doc:\n",
    "    print(\"Token index is {} \\t {:10} \\\n",
    "    alpha: {} \\t  punct: {} \\t \\\n",
    "    number: {}\".format(token.i, token.text, token.is_alpha, token.is_punct, token.like_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97514cc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7adb89ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"In 1990, more than 60% of people in East Asia were in extreme poverty. \"\n",
    "    \"Now less than 4% are.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "506f0e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In\n",
      "1990\n",
      ",\n",
      "more\n",
      "than\n",
      "60\n",
      "%\n",
      "of\n",
      "people\n",
      "in\n",
      "East\n",
      "Asia\n",
      "were\n",
      "in\n",
      "extreme\n",
      "poverty\n",
      ".\n",
      "Now\n",
      "less\n",
      "than\n",
      "4\n",
      "%\n",
      "are\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8ae7abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage found: 60\n",
      "percentage found: 4\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    if token.like_num:\n",
    "        next_token = doc[token.i + 1]\n",
    "        if next_token.text == '%':\n",
    "            print('percentage found: {}'.format(token.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a532ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5e935de",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.3.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.3.0/en_core_web_sm-3.3.0-py3-none-any.whl (12.8 MB)\n",
      "     --------------------------------------- 12.8/12.8 MB 13.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in c:\\users\\sakarimi\\.conda\\envs\\nlp\\lib\\site-packages (from en-core-web-sm==3.3.0) (3.3.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\sakarimi\\.conda\\envs\\nlp\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.7.7)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\sakarimi\\.conda\\envs\\nlp\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\sakarimi\\.conda\\envs\\nlp\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\sakarimi\\.conda\\envs\\nlp\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\sakarimi\\.conda\\envs\\nlp\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.10.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\sakarimi\\.conda\\envs\\nlp\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sakarimi\\.conda\\envs\\nlp\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (65.6.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\sakarimi\\.conda\\envs\\nlp\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.64.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\sakarimi\\.conda\\envs\\nlp\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.7)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sakarimi\\.conda\\envs\\nlp\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.1.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\sakarimi\\.conda\\envs\\nlp\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.7)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\sakarimi\\.conda\\envs\\nlp\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.9.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\sakarimi\\.conda\\envs\\nlp\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sakarimi\\.conda\\envs\\nlp\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (22.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in c:\\users\\sakarimi\\.conda\\envs\\nlp\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.0.15)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\sakarimi\\.conda\\envs\\nlp\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\sakarimi\\.conda\\envs\\nlp\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.8.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\sakarimi\\.conda\\envs\\nlp\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.28.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\sakarimi\\.conda\\envs\\nlp\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.4.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\sakarimi\\.conda\\envs\\nlp\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\sakarimi\\.conda\\envs\\nlp\\lib\\site-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sakarimi\\.conda\\envs\\nlp\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sakarimi\\.conda\\envs\\nlp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sakarimi\\.conda\\envs\\nlp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\sakarimi\\.conda\\envs\\nlp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sakarimi\\.conda\\envs\\nlp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2022.12.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\sakarimi\\.conda\\envs\\nlp\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\sakarimi\\.conda\\envs\\nlp\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sakarimi\\.conda\\envs\\nlp\\lib\\site-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.3.0\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ea92f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained spaCy models, discover more about the words using pretrained models\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b861df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"She ate a big slice of the pizza.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c283687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She        PRON \n",
      "ate        VERB \n",
      "a          DET  \n",
      "big        ADJ  \n",
      "slice      NOUN \n",
      "of         ADP  \n",
      "the        DET  \n",
      "pizza      NOUN \n",
      ".          PUNCT\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(f'{token.text:10} {token.pos_:5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c8ee2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She        PRON       nsubj      ate       \n",
      "ate        VERB       ROOT       ate       \n",
      "a          DET        det        slice     \n",
      "big        ADJ        amod       slice     \n",
      "slice      NOUN       dobj       ate       \n",
      "of         ADP        prep       slice     \n",
      "the        DET        det        pizza     \n",
      "pizza      NOUN       pobj       of        \n",
      ".          PUNCT      punct      ate       \n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(f'{token.text:10} {token.pos_:10} {token.dep_:10} {token.head.text:10}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796d9608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92996a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e3d9870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find about the Named entities in the text, like companies, etc.\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d6f97a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple           ORG\n",
      "U.K.            GPE\n",
      "$1 billion      MONEY\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(f'{ent.text:15} {ent.label_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83bb76f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countries, cities, states\n"
     ]
    }
   ],
   "source": [
    "# find definition of different abbreviations using .explain method\n",
    "print(spacy.explain('GPE'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd10387e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjectival modifier\n"
     ]
    }
   ],
   "source": [
    "print(spacy.explain('amod'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0a54725b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "determiner\n"
     ]
    }
   ],
   "source": [
    "print(spacy.explain('det'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1bb68393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepositional object\n"
     ]
    }
   ],
   "source": [
    "print(spacy.explain('op'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b4ea27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14894eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rule based matching to find in tokenized text\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6cf2d4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b8fea4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [{\"TEXT\": \"iPhone\"}, {\"TEXT\":\"X\"}]\n",
    "matcher.add(\"IPHONE_PATTERN\", [pattern])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b8d6b1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Upcoming iPhone X release date leaked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7feaf1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1f1806a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iPhone X\n"
     ]
    }
   ],
   "source": [
    "for match_id,start,end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "593afa84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018 FIFA World Cup:\n"
     ]
    }
   ],
   "source": [
    "# matching lexical attributes\n",
    "pattern = [{\"IS_DIGIT\": True}, {\"LOWER\": \"fifa\"}, {\"LOWER\": \"world\"}, {\"LOWER\": \"cup\"}, {\"IS_PUNCT\": True}]\n",
    "\n",
    "doc = nlp(\"2018 FIFA World Cup: France won!\")\n",
    "\n",
    "matcher.add(\"FIFA_WORLDCUP\", [pattern])\n",
    "\n",
    "matches = matcher(doc)\n",
    "for match_id,start,stop in matches:\n",
    "    matched_span = doc[start:stop]\n",
    "    print(matched_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef3d11b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d6b8deb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loved dogs\n",
      "love cats\n"
     ]
    }
   ],
   "source": [
    "# matching oter attributes\n",
    "pattern = [{\"LEMMA\": \"love\", \"POS\": \"VERB\"}, {\"POS\": \"NOUN\"}]\n",
    "doc = nlp(\"I loved dogs but now I love cats more.\")\n",
    "matcher.add(\"LOVECATDOGS\", [pattern])\n",
    "\n",
    "matches = matcher(doc)\n",
    "for match_id,start,stop in matches:\n",
    "    matched_span = doc[start:stop]\n",
    "    print(matched_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49a386a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "73bace89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bought a smartphone\n",
      "buying apps\n"
     ]
    }
   ],
   "source": [
    "# matches using operators and qualifiers, OP --> optional: match 0 or 1 times\n",
    "pattern = [{\"LEMMA\": \"buy\"}, {\"POS\": \"DET\", \"OP\": \"?\"}, {\"POS\": \"NOUN\"}]\n",
    "doc = nlp(\"I bought a smartphone. Now I'm buying apps.\")\n",
    "matcher.add(\"BUYSTUFF\", [pattern])\n",
    "\n",
    "matches = matcher(doc)\n",
    "for match_id,start,stop in matches:\n",
    "    matched_span = doc[start:stop]\n",
    "    print(matched_span)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97898371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some of operators and qualifiers\n",
    "# {\"OP\": \"!\"}\tNegation: match 0 times\n",
    "# {\"OP\": \"?\"}\tOptional: match 0 or 1 times\n",
    "# {\"OP\": \"+\"}\tMatch 1 or more times\n",
    "# {\"OP\": \"*\"}\tMatch 0 or more times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0808656a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
